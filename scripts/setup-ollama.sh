#!/bin/bash
# Setup Ollama avec mod√®le L√âGER pour CPX31
# √âvite crash comme sur MacBook (Mistral 7B = 4.3GB RAM)

set -e

echo "üöÄ Setup Ollama pour CPX31 (8GB RAM)..."

# 1. D√©marrer Ollama
echo "1Ô∏è‚É£ D√©marrage Ollama..."
docker compose up -d ollama
sleep 10

# 2. V√©rifier health
echo "2Ô∏è‚É£ Health check..."
docker compose exec ollama curl -f http://localhost:11434/api/tags || {
    echo "‚ùå Ollama pas d√©marr√©"
    exit 1
}

echo "‚úÖ Ollama d√©marr√©"

# 3. T√©l√©charger mod√®le L√âGER (phi ou tinyllama)
echo ""
echo "3Ô∏è‚É£ T√©l√©chargement mod√®le L√âGER..."
echo ""
echo "Choix disponibles (du plus petit au plus gros):"
echo "  1) tinyllama:1.1b  (~637MB)   - Le plus rapide, qualit√© correcte"
echo "  2) phi:2.7b        (~1.6GB)   - Bon compromis"
echo "  3) mistral:7b      (~4.3GB)   - ‚ö†Ô∏è RISQUE CRASH sur CPX31"
echo ""
read -p "Choisir (1/2/3) [1]: " choice
choice=${choice:-1}

case $choice in
    1)
        MODEL="tinyllama:1.1b"
        ;;
    2)
        MODEL="phi:2.7b"
        ;;
    3)
        echo "‚ö†Ô∏è  ATTENTION: mistral:7b peut saturer la RAM du CPX31"
        read -p "Confirmer ? (oui/non) [non]: " confirm
        if [ "$confirm" != "oui" ]; then
            echo "‚ùå Annul√©"
            exit 1
        fi
        MODEL="mistral:7b"
        ;;
    *)
        echo "‚ùå Choix invalide"
        exit 1
        ;;
esac

echo "üì• T√©l√©chargement de $MODEL..."
docker compose exec ollama ollama pull $MODEL

# 4. Tester le mod√®le
echo ""
echo "4Ô∏è‚É£ Test du mod√®le..."
docker compose exec ollama ollama run $MODEL "Bonjour, qui es-tu ?"

echo ""
echo "‚úÖ Ollama configur√© avec succ√®s !"
echo ""
echo "üìä Stats RAM Ollama:"
docker stats ollama --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}"

echo ""
echo "üí° Configuration .env recommand√©e:"
echo "OLLAMA_BASE_URL=http://ollama:11434"
echo "OLLAMA_MODEL=$MODEL"
echo "OLLAMA_ENABLE_FALLBACK=true"
echo "OLLAMA_FALLBACK_MODEL=gpt-3.5-turbo"
