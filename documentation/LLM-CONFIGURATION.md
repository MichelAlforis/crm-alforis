# ü§ñ LLM Configuration Guide

**Date:** 2025-10-28
**Service:** Autofill V2 - LLM Fallback

---

## üìã Overview

L'Autofill V2 utilise un **LLM Router** avec fallback automatique entre 4 providers :
1. **Mistral** (primary, si configur√©)
2. **OpenAI** (fallback #1)
3. **Anthropic** (fallback #2)
4. **Ollama** (fallback #3, local)

Le router g√®re automatiquement :
- ‚úÖ Circuit breaker (bloque provider apr√®s N √©checs)
- ‚úÖ Retry avec backoff exponentiel
- ‚úÖ Cache TTL (60s par d√©faut)
- ‚úÖ Cost cap par requ√™te (0.10‚Ç¨ par d√©faut)
- ‚úÖ Timeout configurable

---

## üîë Environment Variables

### Providers Configuration

```bash
# ===== Mistral (recommand√©) =====
MISTRAL_API_KEY=your_mistral_api_key
MISTRAL_API_BASE=https://api.mistral.ai  # Default
LLM_PRIMARY_MODEL=mistral-large-latest    # ou mistral-medium

# ===== OpenAI =====
OPENAI_API_KEY=your_openai_api_key
OPENAI_MODEL=gpt-4o-mini                  # ou gpt-4o
OPENAI_MAX_TOKENS=500

# ===== Anthropic =====
ANTHROPIC_API_KEY=your_anthropic_api_key
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022  # ou claude-3-opus
ANTHROPIC_MAX_TOKENS=500

# ===== Ollama (local, optionnel) =====
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=mistral:7b
```

---

### Router Configuration

```bash
# ===== Provider Selection =====
LLM_PRIMARY_PROVIDER=auto              # ou "mistral", "openai", "anthropic", "ollama"
LLM_SECONDARY_PROVIDER=auto            # Fallback si primary √©choue
LLM_PROVIDER_ORDER=mistral,openai,anthropic,ollama  # Ordre de fallback

# ===== Performance =====
LLM_LATENCY_BUDGET_MS=600              # Budget latence max pour autofill
LLM_REQUEST_TIMEOUT_MS=10000           # Timeout HTTP (10s)
LLM_TIMEOUT_MS=8000                    # Timeout global provider

# ===== Circuit Breaker =====
LLM_CIRCUIT_BREAKER_THRESHOLD=3        # Bloque provider apr√®s 3 √©checs
LLM_CIRCUIT_BREAKER_COOLDOWN_SECONDS=60  # Cooldown avant retry

# ===== Cost & Retry =====
LLM_COST_CAP_EUR=0.10                  # Cap co√ªt par requ√™te (‚Ç¨)
LLM_MAX_RETRIES=1                      # Retry max avant fallback

# ===== Cache =====
LLM_CACHE_TTL_SECONDS=60               # Cache r√©ponses identiques (60s)
```

---

## üöÄ Configuration Recommand√©e par Environnement

### Development (co√ªt z√©ro)

```bash
# Pas de LLM en dev ‚Üí autofill utilise rules + DB patterns uniquement
# Ou Ollama local si tu veux tester
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral:7b
LLM_PRIMARY_PROVIDER=ollama
```

### Staging (budget limit√©)

```bash
# OpenAI primary, pas de fallback co√ªteux
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini              # Pas cher (~$0.15/1M tokens)
LLM_PRIMARY_PROVIDER=openai
LLM_COST_CAP_EUR=0.05                 # Cap √† 5 centimes/requ√™te
LLM_LATENCY_BUDGET_MS=600
```

### Production (fiabilit√© max)

```bash
# Mistral primary, OpenAI fallback, Anthropic fallback #2
MISTRAL_API_KEY=...
MISTRAL_API_BASE=https://api.mistral.ai
LLM_PRIMARY_MODEL=mistral-large-latest

OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini

ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

LLM_PRIMARY_PROVIDER=mistral
LLM_SECONDARY_PROVIDER=openai
LLM_PROVIDER_ORDER=mistral,openai,anthropic

LLM_COST_CAP_EUR=0.10
LLM_LATENCY_BUDGET_MS=600
LLM_MAX_RETRIES=1
LLM_CIRCUIT_BREAKER_THRESHOLD=3
LLM_CACHE_TTL_SECONDS=60
```

---

## üí∞ Co√ªt Estimation

### Tarifs providers (per 1M tokens input/output)

| Provider  | Model                        | Input  | Output | Moyen/req |
| --------- | ---------------------------- | ------ | ------ | --------- |
| Mistral   | mistral-large-latest         | $2     | $6     | ~$0.003   |
| Mistral   | mistral-medium-latest        | $0.7   | $2.1   | ~$0.001   |
| OpenAI    | gpt-4o-mini                  | $0.15  | $0.60  | ~$0.0003  |
| OpenAI    | gpt-4o                       | $2.50  | $10    | ~$0.005   |
| Anthropic | claude-3-5-sonnet-20241022   | $3     | $15    | ~$0.007   |
| Anthropic | claude-3-opus                | $15    | $75    | ~$0.035   |
| Ollama    | mistral:7b (local)           | FREE   | FREE   | $0        |

**Note:** Moyen/req bas√© sur ~500 tokens input + 200 tokens output pour autofill.

### Budget mensuel recommand√©

- **Staging:** 100‚Ç¨/mois ‚Üí ~33k requ√™tes LLM (avec gpt-4o-mini)
- **Production:** 500‚Ç¨/mois ‚Üí ~50k requ√™tes LLM (avec mistral-large)
  - Avec cache 60s + rules/DB patterns, ~70% des autofills n'appellent pas le LLM
  - Donc 50k LLM calls = ~170k autofills totaux

---

## üîç Monitoring & Alerting

### M√©triques √† surveiller

1. **Fallback rate**: % d'appels qui utilisent le fallback
   - ‚úÖ < 5% : normal (timeouts occasionnels)
   - ‚ö†Ô∏è 5-10% : surveiller
   - üî¥ > 10% : alerter (provider primary en panne)

2. **Cost per day**: Co√ªt total LLM/jour
   - ‚úÖ < 5‚Ç¨/jour : normal staging
   - ‚ö†Ô∏è 5-20‚Ç¨/jour : surveiller
   - üî¥ > 20‚Ç¨/jour : alerter (spam ou bug)

3. **Cache hit rate**: % d'appels servis depuis cache
   - ‚úÖ > 30% : bon
   - ‚ö†Ô∏è 10-30% : acceptable
   - üî¥ < 10% : cache inefficace

4. **Latency p95**: 95e percentile de latence LLM
   - ‚úÖ < 600ms : excellent
   - ‚ö†Ô∏è 600-1000ms : acceptable
   - üî¥ > 1000ms : trop lent

### Grafana Dashboard (√† cr√©er - Phase 3)

```sql
-- Fallback rate (Prometheus)
sum(rate(llm_fallback_total[5m])) / sum(rate(llm_requests_total[5m]))

-- Cost per day
sum(increase(llm_cost_eur[24h]))

-- Cache hit rate
sum(rate(llm_cache_hits[5m])) / sum(rate(llm_requests_total[5m]))

-- Latency p95
histogram_quantile(0.95, rate(llm_latency_ms_bucket[5m]))
```

---

## üß™ Testing LLM Router

### Test manuel en dev

```bash
# 1. V√©rifier configuration
docker-compose exec api python3 -c "
from services.llm_router import LLMRouter
router = LLMRouter()
print(router.describe())
"

# 2. Test appel LLM (n√©cessite OPENAI_API_KEY ou autre)
docker-compose exec api python3 -c "
import asyncio
from services.llm_router import LLMRouter

async def test():
    router = LLMRouter()
    result = await router.generate_autofill_context(
        draft={'raw_snippet': 'John Doe | john@example.com | +33 6 12 34 56 78'},
        suggestions={}
    )
    print(f'Provider: {result.provider}')
    print(f'Latency: {result.latency_ms}ms')
    print(f'Cost: {result.cost_eur}‚Ç¨')
    print(f'Content: {result.content[:100]}...')

asyncio.run(test())
"
```

### Test avec donn√©es r√©elles

```bash
# D√©clencher autofill sur un contact
curl -X POST http://localhost:8000/api/v1/ai/autofill \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "entity_type": "person",
    "draft": {
      "raw_snippet": "Jean Dupont | Directeur Marketing | jean.dupont@mandarine-gestion.com | +33 1 23 45 67 89",
      "first_name": "Jean",
      "last_name": "Dupont"
    },
    "context": {
      "budget_mode": "normal"
    }
  }'
```

---

## üîí Security Best Practices

1. **API Keys Storage**
   - ‚úÖ Stocke les cl√©s dans des secrets (Kubernetes Secrets, AWS Secrets Manager)
   - ‚ùå **JAMAIS** commit les cl√©s dans le code ou .env versionn√©
   - ‚úÖ Utilise des variables d'env avec rotation r√©guli√®re

2. **Rate Limiting**
   - ‚úÖ Limite 10 req/min par user sur `/ai/autofill`
   - ‚úÖ Cost cap par requ√™te (0.10‚Ç¨ max)
   - ‚úÖ Circuit breaker pour √©viter spam si provider down

3. **Logging**
   - ‚úÖ Log les m√©triques (provider, latency, cost, tokens)
   - ‚ùå **NE PAS** logger les API keys ou les prompts complets (RGPD)
   - ‚úÖ Hash les payloads pour cache (SHA256)

4. **Fallback Graceful**
   - ‚úÖ Si tous les providers √©chouent, autofill fonctionne quand m√™me (rules + DB patterns)
   - ‚úÖ Pas d'erreur 500, juste un warning dans les logs

---

## üìä Decision act√©e (2025-10-28)

**Configuration initiale prod:**
```bash
LLM_PRIMARY_PROVIDER=auto
OPENAI_API_KEY=sk-...           # Fourni
OPENAI_MODEL=gpt-4o-mini
ANTHROPIC_API_KEY=sk-ant-...    # Fourni
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

LLM_LATENCY_BUDGET_MS=600
LLM_REQUEST_TIMEOUT_MS=10000
LLM_MAX_RETRIES=1
LLM_COST_CAP_EUR=0.05           # 5 centimes/requ√™te
LLM_CIRCUIT_BREAKER_THRESHOLD=3
LLM_CACHE_TTL_SECONDS=60
```

**Budget mensuel initial:** 100‚Ç¨ (ajustable apr√®s 1 semaine d'observation)

**Alerte si:** Co√ªt/jour > 5‚Ç¨ ou fallback rate > 10%

---

## üîó Liens Utiles

- [Mistral Pricing](https://mistral.ai/technology/#pricing)
- [OpenAI Pricing](https://openai.com/pricing)
- [Anthropic Pricing](https://www.anthropic.com/pricing)
- [Ollama Models](https://ollama.ai/library)

---

**Pr√™t pour Phase 1 ‚úÖ**
