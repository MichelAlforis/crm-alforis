# ===========================================
# DOCKER COMPOSE - PRODUCTION
# ===========================================
# Configuration optimisée pour la production
# - Chargement automatique de .env (natif Docker Compose)
# - Build optimisé avec multi-stage
# - Healthchecks robustes
# - Logs limités avec rotation
# - Variables d'environnement sécurisées
# - Monitoring avec Prometheus + Grafana
# - Tâches asynchrones avec Celery + Flower
# - AI Local avec Ollama (mistral:7b)

# IMPORTANT: Créer un fichier .env à la racine avec:
# - POSTGRES_PASSWORD (obligatoire)
# - SECRET_KEY (obligatoire)
# - ENCRYPTION_KEY (obligatoire)
# - EMAIL_ENCRYPTION_KEY (obligatoire)
# - GRAFANA_ADMIN_PASSWORD (obligatoire)
# - FLOWER_BASIC_AUTH (obligatoire, format: user:password)
# - ALLOWED_ORIGINS (optionnel, défaut: ["https://crm.alforis.fr"])
# - NEXT_PUBLIC_API_URL (optionnel, défaut: /api/v1)
# - MICROSOFT_CLIENT_ID, MICROSOFT_CLIENT_SECRET (optionnel, pour Outlook)
# - SERPAPI_API_KEY (optionnel, pour enrichissement web)

# --- Réseau ---
networks:
  crm-network:
    driver: bridge

# --- Volumes ---
volumes:
  postgres-data:
    driver: local
  redis-data:
    driver: local
  api-uploads:
    driver: local
  api-backups:
    driver: local
  api-logs:
    driver: local
  caddy-data:
    driver: local
  caddy-config:
    driver: local
  caddy-logs:
    driver: local
  ollama-models:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

# --- Services ---
services:

  # ========================================
  # CADDY (Reverse Proxy + SSL/TLS Auto)
  # ========================================
  caddy:
    image: caddy:2.7-alpine
    restart: always
    ports:
      - "80:80"      # HTTP (redirect to HTTPS)
      - "443:443"    # HTTPS
      - "443:443/udp"  # HTTP/3 (QUIC)
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy-data:/data
      - caddy-config:/config
      - caddy-logs:/var/log/caddy
    networks:
      - crm-network
    depends_on:
      - api
      - frontend
    environment:
      - DOMAIN=${DOMAIN:-crm.alforis.fr}
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:2019/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # REDIS (Cache & Celery Broker)
  # ========================================
  redis:
    image: redis:7-alpine
    restart: always
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru --appendonly yes
    ports:
      - "127.0.0.1:${REDIS_PORT:-6379}:6379"
    volumes:
      - redis-data:/data
    networks:
      - crm-network
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # POSTGRES
  # ========================================
  postgres:
    image: postgres:16-alpine
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-crm_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD doit être défini dans .env}
      POSTGRES_DB: ${POSTGRES_DB:-crm_db}
      PGDATA: /var/lib/postgresql/data/pgdata
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=128MB"
      - "-c"
      - "work_mem=4MB"
      - "-c"
      - "maintenance_work_mem=64MB"
      - "-c"
      - "effective_cache_size=512MB"
      - "-c"
      - "max_connections=50"
    ports:
      # Utiliser un port différent pour éviter les conflits avec PostgreSQL local
      - "127.0.0.1:${POSTGRES_EXTERNAL_PORT:-5433}:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - crm-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h 127.0.0.1 -p 5432 -U ${POSTGRES_USER:-crm_user} -d postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # API (FastAPI)
  # ========================================
  api:
    build:
      context: ./crm-backend
      dockerfile: Dockerfile
      target: production
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    # Docker Compose charge automatiquement .env à la racine
    environment:
      # URL de connexion PostgreSQL (utilise le nom du service 'postgres')
      DATABASE_URL: postgresql://${POSTGRES_USER:-crm_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-crm_db}
      # Redis Cache & Event Bus
      REDIS_ENABLED: "True"
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_DB: 0
      # Ollama (AI Local LLM)
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_MODEL: ${OLLAMA_MODEL:-mistral:7b}
      # Sécurité
      DEBUG: "False"
      SECRET_KEY: ${SECRET_KEY:?SECRET_KEY doit être défini dans .env}
      ALLOWED_ORIGINS: ${ALLOWED_ORIGINS:-["https://crm.alforis.fr"]}
      # JWT
      JWT_ALGORITHM: ${JWT_ALGORITHM:-HS256}
      JWT_EXPIRATION_HOURS: ${JWT_EXPIRATION_HOURS:-24}
      # AI Agent - Encryption
      ENCRYPTION_KEY: ${ENCRYPTION_KEY:?ENCRYPTION_KEY doit être défini dans .env}
      EMAIL_ENCRYPTION_KEY: ${EMAIL_ENCRYPTION_KEY:?EMAIL_ENCRYPTION_KEY doit être défini dans .env}
      # Microsoft OAuth (Outlook)
      MICROSOFT_CLIENT_ID: ${MICROSOFT_CLIENT_ID}
      MICROSOFT_CLIENT_SECRET: ${MICROSOFT_CLIENT_SECRET}
      MICROSOFT_REDIRECT_URI: ${MICROSOFT_REDIRECT_URI}
      # Web Enrichment
      SERPAPI_API_KEY: ${SERPAPI_API_KEY}
      # Webhooks
      WEBHOOK_SECRET: ${WEBHOOK_SECRET}
      UNSUBSCRIBE_JWT_SECRET: ${UNSUBSCRIBE_JWT_SECRET}
      # Environnement
      ENVIRONMENT: production
      MAX_UPLOAD_SIZE_MB: ${MAX_UPLOAD_SIZE_MB:-10}
      API_PORT: 8000
    working_dir: /app
    # Production: 4 workers, pas de logs d'accès pour les perfs
    command: uvicorn main:app --host 0.0.0.0 --port ${API_PORT:-8000} --proxy-headers
    ports:
      - "127.0.0.1:${API_PORT:-8000}:8000"
    volumes:
      - api-uploads:/app/uploads
      - api-backups:/app/backups
      - api-logs:/app/logs
    networks:
      - crm-network
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request;urllib.request.urlopen(\"http://localhost:8000/api/v1/health\")' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # ========================================
  # FRONTEND (Next.js)
  # ========================================
  frontend:
    build:
      context: ./crm-frontend
      dockerfile: Dockerfile
      target: production
      args:
        # Passer l'URL de l'API au build - Lit depuis .env racine
        NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-/api/v1}
    restart: always
    depends_on:
      api:
        condition: service_healthy
    # Docker Compose charge automatiquement .env à la racine
    environment:
      # URL de l'API accessible depuis le navigateur
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-/api/v1}
      NODE_ENV: production
      PORT: ${FRONTEND_PORT:-3010}
    working_dir: /app
    command: npm start
    ports:
      - "127.0.0.1:${FRONTEND_PORT:-3010}:${FRONTEND_PORT:-3010}"
    networks:
      - crm-network
    healthcheck:
      # Healthcheck plus robuste qui vérifie vraiment la disponibilité
      test: ["CMD-SHELL", "node -e 'require(\"http\").get(\"http://localhost:${FRONTEND_PORT:-3010}\",r=>process.exit(r.statusCode>=200&&r.statusCode<500?0:1)).on(\"error\",()=>process.exit(1))'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # ========================================
  # OLLAMA (Local LLM for AI features)
  # ========================================
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "127.0.0.1:11434:11434"
    networks:
      - crm-network
    volumes:
      - ollama-models:/root/.ollama
    environment:
      # Config optimisée pour production
      OLLAMA_MAX_LOADED_MODELS: 1           # 1 modèle en mémoire
      OLLAMA_NUM_PARALLEL: 2                # 2 requêtes parallèles
      OLLAMA_MAX_QUEUE: 5                   # Queue jusqu'à 5 requêtes
      OLLAMA_KEEP_ALIVE: 5m                 # Garde modèle 5min en RAM
    deploy:
      resources:
        limits:
          cpus: '2.0'      # 2 CPU sur 4 (50%)
          memory: 5G       # 5GB max
        reservations:
          cpus: '1.0'      # Démarre avec 1 CPU
          memory: 2G       # 2GB minimum
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # CELERY WORKER (Background Tasks)
  # ========================================
  celery-worker:
    build:
      context: ./crm-backend
      dockerfile: Dockerfile
      target: production
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-crm_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-crm_db}
      REDIS_URL: redis://redis:6379/0
      REDIS_ENABLED: "True"
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_DB: 0
      SECRET_KEY: ${SECRET_KEY:?SECRET_KEY doit être défini dans .env}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY:?ENCRYPTION_KEY doit être défini dans .env}
      EMAIL_ENCRYPTION_KEY: ${EMAIL_ENCRYPTION_KEY:?EMAIL_ENCRYPTION_KEY doit être défini dans .env}
      ENVIRONMENT: production
      POSTGRES_USER: ${POSTGRES_USER:-crm_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-crm_db}
      POSTGRES_HOST: postgres
      PYTHONPATH: /app
    working_dir: /app
    command: celery -A celery_app worker --loglevel=info --concurrency=2 --max-tasks-per-child=100
    volumes:
      - api-logs:/app/logs
    networks:
      - crm-network
    healthcheck:
      test: ["CMD-SHELL", "celery -A celery_app inspect ping || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # CELERY BEAT (Periodic Task Scheduler)
  # ========================================
  celery-beat:
    build:
      context: ./crm-backend
      dockerfile: Dockerfile
      target: production
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      celery-worker:
        condition: service_started
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-crm_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-crm_db}
      REDIS_URL: redis://redis:6379/0
      REDIS_ENABLED: "True"
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_DB: 0
      SECRET_KEY: ${SECRET_KEY:?SECRET_KEY doit être défini dans .env}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY:?ENCRYPTION_KEY doit être défini dans .env}
      EMAIL_ENCRYPTION_KEY: ${EMAIL_ENCRYPTION_KEY:?EMAIL_ENCRYPTION_KEY doit être défini dans .env}
      ENVIRONMENT: production
      POSTGRES_USER: ${POSTGRES_USER:-crm_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-crm_db}
      POSTGRES_HOST: postgres
      PYTHONPATH: /app
    working_dir: /app
    command: celery -A celery_app beat --loglevel=info --schedule=/tmp/celerybeat-schedule
    volumes:
      - api-logs:/app/logs
    networks:
      - crm-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # FLOWER (Celery Monitoring UI)
  # ========================================
  flower:
    build:
      context: ./crm-backend
      dockerfile: Dockerfile
      target: production
    restart: always
    depends_on:
      redis:
        condition: service_healthy
      celery-worker:
        condition: service_started
    environment:
      REDIS_URL: redis://redis:6379/0
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      DATABASE_URL: postgresql://${POSTGRES_USER:-crm_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-crm_db}
      POSTGRES_USER: ${POSTGRES_USER:-crm_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-crm_db}
      POSTGRES_HOST: postgres
      PYTHONPATH: /app
      # Flower Basic Auth (obligatoire en prod)
      FLOWER_BASIC_AUTH: ${FLOWER_BASIC_AUTH:?FLOWER_BASIC_AUTH doit être défini dans .env (format: user:password)}
    working_dir: /app
    command: celery -A celery_app flower --port=5555 --basic-auth=${FLOWER_BASIC_AUTH}
    ports:
      - "127.0.0.1:5555:5555"
    networks:
      - crm-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:5555 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # PROMETHEUS (Metrics Collection & Scraping)
  # ========================================
  prometheus:
    image: prom/prometheus:latest
    restart: always
    depends_on:
      api:
        condition: service_started
    ports:
      - "127.0.0.1:9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - crm-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # GRAFANA (Metrics Visualization & Dashboards)
  # ========================================
  grafana:
    image: grafana/grafana:latest
    restart: always
    depends_on:
      prometheus:
        condition: service_started
    ports:
      - "127.0.0.1:3001:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:?GRAFANA_ADMIN_PASSWORD doit être défini dans .env}
      GF_SERVER_ROOT_URL: ${GRAFANA_ROOT_URL:-https://crm.alforis.fr/grafana}
      GF_INSTALL_PLUGINS: ""
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - crm-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
